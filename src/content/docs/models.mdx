---
title: Available Models
description: Coding models available on CodeVault
order: 6
---

# Available Models

CodeVault supports a wide range of open-source coding models. We deploy and optimize the models based on your hardware and requirements.

## Featured Models

### DeepSeek V3

- **Parameters:** 685B MoE (37B active)
- **Context:** 128K tokens
- **Best for:** Complex reasoning, multi-file understanding, agentic tasks

State-of-the-art coding model with top benchmark scores. Excellent for complex tasks requiring deep understanding.

### Qwen 3

- **Parameters:** 32B
- **Context:** 128K tokens
- **Best for:** Code generation, reasoning, multi-language support

Alibaba's latest model with hybrid thinking capabilities. Great balance of speed and intelligence.

### Devstral

- **Parameters:** 24B
- **Context:** 128K tokens
- **Best for:** Agentic coding, SWE-Bench tasks

Mistral's specialized coding agent. Optimized for autonomous coding tasks.

### DeepSeek Coder V2

- **Parameters:** 16B / 236B MoE
- **Context:** 128K tokens
- **Best for:** Polyglot development (338 languages)

Excellent multi-language support with strong performance across all programming languages.

## Model Selection

Choose models based on your needs:

| Use Case | Recommended Model |
|----------|-------------------|
| Complex reasoning | DeepSeek V3 |
| Fast completions | Qwen 3 (32B) |
| Agentic tasks | Devstral |
| Multi-language | DeepSeek Coder V2 |

## Custom Models

Want a model not listed here? We can deploy any open-source model:

- **Llama 3.x** - Meta's open models
- **CodeLlama** - Meta's code-specialized models
- **StarCoder 2** - BigCode's coding models
- **Codestral** - Mistral's code model
- **And more...**

Contact us to discuss your model requirements.

## Hardware Requirements

Model requirements vary by size:

| Model Size | VRAM Required | Recommended GPU |
|------------|---------------|-----------------|
| 7B | 8GB | RTX 3080/4080 |
| 14-16B | 16GB | RTX 3090/4090 |
| 32B | 24GB+ | RTX 4090 / A100 |
| 70B+ | 48GB+ | A100 / H100 |
| MoE (V3) | 80GB+ | Multi-GPU / H100 |

We help you select the right hardware configuration during deployment planning.
